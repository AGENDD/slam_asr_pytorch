{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "# model = AutoModel.from_pretrained(\"facebook/hubert-base-ls960\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kinet\\anaconda3\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for hf-internal-testing/librispeech_asr_dummy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hf-internal-testing/librispeech_asr_dummy\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "ds = ds.map(map_to_array)\n",
    "\n",
    "# batch_inputs = processor(ds[\"speech\"][0:3], return_tensors=\"pt\", padding=True)\n",
    "# hidden_states = model(**batch_inputs).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kinet\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from modeling.speech_encoder import SpeechEncoder\n",
    "\n",
    "speech_encoder = SpeechEncoder(\"facebook/hubert-base-ls960\", 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "x = speech_encoder(ds[\"speech\"][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 124, 1024]), torch.Size([3, 124]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape, x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2111, -0.0168,  0.0062,  ...,  0.0667,  0.0676,  0.0423],\n",
       "         [ 0.0717,  0.0738,  0.0336,  ...,  0.0785,  0.0360,  0.0960],\n",
       "         [ 0.0540, -0.0797, -0.0423,  ...,  0.1826,  0.1093,  0.0815],\n",
       "         ...,\n",
       "         [-0.1273, -0.0323,  0.0087,  ..., -0.0300,  0.1291, -0.0452],\n",
       "         [-0.1273, -0.0323,  0.0087,  ..., -0.0300,  0.1291, -0.0452],\n",
       "         [-0.1273, -0.0323,  0.0087,  ..., -0.0300,  0.1291, -0.0452]],\n",
       "\n",
       "        [[ 0.2582, -0.0732, -0.0400,  ...,  0.0500,  0.0669,  0.0614],\n",
       "         [ 0.0977, -0.0165, -0.0305,  ..., -0.0058,  0.0943,  0.1681],\n",
       "         [ 0.0829,  0.0346, -0.0634,  ..., -0.0404,  0.0103,  0.1833],\n",
       "         ...,\n",
       "         [-0.1704,  0.0190,  0.0266,  ..., -0.0338,  0.0625, -0.0431],\n",
       "         [-0.1704,  0.0190,  0.0266,  ..., -0.0338,  0.0625, -0.0431],\n",
       "         [-0.1704,  0.0190,  0.0266,  ..., -0.0338,  0.0625, -0.0431]],\n",
       "\n",
       "        [[ 0.2396, -0.0706, -0.0021,  ...,  0.0750,  0.0578,  0.0155],\n",
       "         [ 0.0786, -0.0516, -0.0093,  ...,  0.1028,  0.0704,  0.1349],\n",
       "         [ 0.0064, -0.0034,  0.0262,  ...,  0.1040,  0.0952,  0.1375],\n",
       "         ...,\n",
       "         [ 0.0738, -0.0912, -0.1373,  ..., -0.0142,  0.1690,  0.0699],\n",
       "         [ 0.1776, -0.0746, -0.0871,  ..., -0.0423,  0.0842,  0.0083],\n",
       "         [ 0.0945, -0.1110, -0.1694,  ..., -0.0829,  0.2043,  0.0519]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v0.4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "CHAT_EOS_TOKEN_ID = 32002\n",
    "\n",
    "prompt = \"How to get in a good university?\"\n",
    "formatted_prompt = (\n",
    "    f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "sequences = pipeline(\n",
    "    formatted_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p = 0.9,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n",
    "processor = Speech2Text2Processor.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n",
    "\n",
    "\n",
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch\n",
    "\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "ds = ds.map(map_to_array)\n",
    "\n",
    "inputs = processor(ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|im_start|>user\\n\"\"\"\n",
    "formatted_prompt = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1919,  1301, 29581,   278, 10348, 29889, 32002,    13, 32001,\n",
       "           465, 22137,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
